{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/train.csv\n",
      "/kaggle/input/tf-roberta/vocab-roberta-base.json\n",
      "/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n",
      "/kaggle/input/tf-roberta/merges-roberta-base.txt\n",
      "/kaggle/input/tf-roberta/config-roberta-base.json\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers) (0.0.43)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers) (0.1.86)\r\n",
      "Requirement already satisfied: tokenizers==0.7.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.0.10)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.23.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.45.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers) (1.18.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2020.4.4)\r\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (7.1.1)\r\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (1.14.0)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers) (0.14.1)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.24.3)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.9)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2020.4.5.1)\r\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.7/site-packages (0.7.0)\r\n",
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -3]\n",
      "[nltk_data]     Temporary failure in name resolution>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tokenizers\n",
    "############################imports##########################################\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "\n",
    "import re\n",
    "import string\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab_file=PATH+'vocab-roberta-base.json', \n",
    "    merges_file=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "sample_submission = pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded=dict({'input_id':[],\n",
    "                      'attention_mask':[],\n",
    "                      'token_id':[],\n",
    "                      'start_token':[],\n",
    "                      'end_token':[]})\n",
    "\n",
    "    #single_sentence=single_sentence_raw.copy()\n",
    "   \n",
    "for index in range(train.shape[0]):\n",
    "    input_id=np.ones((MAX_LEN),dtype='int32');attention_mask=np.zeros((MAX_LEN),dtype='int32');token_id=np.zeros((MAX_LEN),dtype='int32')\n",
    "    start_token=np.zeros((MAX_LEN),dtype='int32');end_token=np.zeros((MAX_LEN),dtype='int32')\n",
    "   \n",
    "    text1 = \" \"+\" \".join(train.loc[index,'text'].split())\n",
    "    text2 = \" \".join(train.loc[index,'selected_text'].split())\n",
    "    #print(\"text1 : \",text1)\n",
    "    #print(\"text2 : \",text2)\n",
    "\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': \n",
    "      chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "    # ID_OFFSETS\n",
    "    #print(enc)\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    #print(\"offsets=\",offsets)\n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "    s_tok = sentiment_id[train.loc[index,'sentiment']]\n",
    "    input_id[0:len(enc.ids)+5]=[0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[0:len(enc.ids)+5]=1\n",
    "    if len(toks)>0:\n",
    "      start_token[toks[0]+1]=1\n",
    "      end_token[toks[-1]+1]=1\n",
    "    \n",
    "    train_encoded['input_id'].append(input_id)\n",
    "    train_encoded['attention_mask'].append(attention_mask)\n",
    "    train_encoded['start_token'].append(start_token)\n",
    "    train_encoded['end_token'].append(end_token)\n",
    "    train_encoded['token_id'].append(token_id)\n",
    "    #print(global()[col])\n",
    "for col in list(train_encoded.keys()):\n",
    "  train_encoded[col]=np.array(train_encoded[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded=dict({'input_id':[],\n",
    "                      'attention_mask':[],\n",
    "                      'token_id':[]})\n",
    "\n",
    "\n",
    "for index in range(test.shape[0]):\n",
    "    input_id=np.ones((MAX_LEN),dtype='int32');attention_mask=np.zeros((MAX_LEN),dtype='int32');token_id=np.zeros((MAX_LEN),dtype='int32')\n",
    "    text1 = \" \"+\" \".join(test.loc[index,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[index,'sentiment']]\n",
    "    input_id[0:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[0:len(enc.ids)+5] = 1\n",
    "    test_encoded['input_id'].append(input_id)\n",
    "    test_encoded['attention_mask'].append(attention_mask)\n",
    "    test_encoded['token_id'].append(token_id)\n",
    "    #print(global()[col])\n",
    "for col in list(test_encoded.keys()):\n",
    "  test_encoded[col]=np.array(test_encoded[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.15)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(768,2,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.LeakyReLU()(x1)\n",
    "    x1 = tf.keras.layers.Conv1D(64, 1,padding='same')(x1)\n",
    "    x1 = tf.keras.layers.Dense(1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(768,2,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.LeakyReLU()(x2)\n",
    "    x2 = tf.keras.layers.Conv1D(64, 1,padding='same')(x2)\n",
    "    x2 = tf.keras.layers.Dense(1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model\n",
    "#jaccard\n",
    "\n",
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(df,preds,index,isTrain=True):\n",
    "    all=[];sel_text=[]\n",
    "    for i in index:\n",
    "        start_tok=np.argmax(preds['start_token'][i])\n",
    "        end_tok=np.argmax(preds['end_token'][i])\n",
    "        if(start_tok>end_tok):\n",
    "            temp_pred = df.loc[i,'text']\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(df.loc[i,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            temp_pred = tokenizer.decode(enc.ids[start_tok-1:end_tok])\n",
    "        if(isTrain):\n",
    "            all.append(jaccard(temp_pred,df.loc[i,'selected_text']))\n",
    "        sel_text.append(temp_pred)\n",
    "\n",
    "    print(np.mean(all))\n",
    "\n",
    "    return sel_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "  \n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    \n",
    "    x1_p = tf.keras.layers.Dropout(0.15)(x[0]) \n",
    "    x1_p = tf.keras.layers.Conv1D(768,4,padding='same')(x1_p)\n",
    "    x1_p = tf.keras.layers.LeakyReLU()(x1_p)\n",
    "    x1_p = tf.keras.layers.Conv1D(512,2,padding='same')(x1_p)\n",
    "    x1_p = tf.keras.layers.LeakyReLU()(x1_p)\n",
    "    x1_p = tf.keras.layers.AveragePooling1D(pool_size=2,padding='same',data_format='channels_first')(x1_p)\n",
    "    x1_p = tf.keras.layers.Dropout(0.1)(x1_p)\n",
    "    x1_p = tf.keras.layers.Conv1D(256,1,padding='same')(x1_p)\n",
    "    x1_p = tf.keras.layers.LeakyReLU()(x1_p)\n",
    "    x1_p = tf.keras.layers.Conv1D(64, 1,padding='same')(x1_p)\n",
    "    x1_p = tf.keras.layers.Dense(1)(x1_p)\n",
    "    x1_p = tf.keras.layers.Flatten()(x1_p)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1_p)\n",
    "\n",
    "    x2_p = tf.keras.layers.Dropout(0.15)(x[0]) \n",
    "    x2_p = tf.keras.layers.Conv1D(768,4,padding='same')(x2_p)\n",
    "    x2_p = tf.keras.layers.LeakyReLU()(x2_p)\n",
    "    x2_p = tf.keras.layers.Conv1D(512,2,padding='same')(x2_p)\n",
    "    x2_p = tf.keras.layers.LeakyReLU()(x2_p)\n",
    "    x2_p = tf.keras.layers.AveragePooling1D(pool_size=2,padding='same',data_format='channels_first')(x2_p)\n",
    "    x2_p = tf.keras.layers.Dropout(0.1)(x2_p)\n",
    "    x2_p = tf.keras.layers.Conv1D(256,1,padding='same')(x2_p)\n",
    "    x2_p = tf.keras.layers.LeakyReLU()(x2_p)\n",
    "    x2_p = tf.keras.layers.Conv1D(64, 1,padding='same')(x2_p)\n",
    "    x2_p = tf.keras.layers.Dense(1)(x2_p)\n",
    "    x2_p = tf.keras.layers.Flatten()(x2_p)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2_p)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n",
      "Train on 21984 samples, validate on 5497 samples\n",
      "Epoch 1/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 2.0498 - activation_7_loss: 1.0621 - activation_8_loss: 0.9877\n",
      "Epoch 00001: val_loss improved from inf to 1.73662, saving model to 1-roberta-0.h5\n",
      "21984/21984 [==============================] - 308s 14ms/sample - loss: 2.0488 - activation_7_loss: 1.0615 - activation_8_loss: 0.9873 - val_loss: 1.7366 - val_activation_7_loss: 0.8956 - val_activation_8_loss: 0.8407\n",
      "Epoch 2/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.5938 - activation_7_loss: 0.8210 - activation_8_loss: 0.7728\n",
      "Epoch 00002: val_loss improved from 1.73662 to 1.64879, saving model to 1-roberta-0.h5\n",
      "21984/21984 [==============================] - 290s 13ms/sample - loss: 1.5938 - activation_7_loss: 0.8213 - activation_8_loss: 0.7725 - val_loss: 1.6488 - val_activation_7_loss: 0.8488 - val_activation_8_loss: 0.8000\n",
      "Epoch 3/3\n",
      "21952/21984 [============================>.] - ETA: 0s - loss: 1.4383 - activation_7_loss: 0.7396 - activation_8_loss: 0.6987\n",
      "Epoch 00003: val_loss did not improve from 1.64879\n",
      "21984/21984 [==============================] - 288s 13ms/sample - loss: 1.4393 - activation_7_loss: 0.7400 - activation_8_loss: 0.6993 - val_loss: 1.6996 - val_activation_7_loss: 0.8873 - val_activation_8_loss: 0.8120\n",
      "Predicting OOF...\n",
      "5497/5497 [==============================] - 29s 5ms/sample\n",
      "0.699217201948777\n",
      "3534/3534 [==============================] - 16s 5ms/sample\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.0975 - activation_9_loss: 1.0706 - activation_10_loss: 1.0269\n",
      "Epoch 00001: val_loss improved from inf to 1.67814, saving model to 1-roberta-1.h5\n",
      "21985/21985 [==============================] - 323s 15ms/sample - loss: 2.0975 - activation_9_loss: 1.0701 - activation_10_loss: 1.0265 - val_loss: 1.6781 - val_activation_9_loss: 0.8747 - val_activation_10_loss: 0.8026\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6189 - activation_9_loss: 0.8338 - activation_10_loss: 0.7851\n",
      "Epoch 00002: val_loss improved from 1.67814 to 1.62634, saving model to 1-roberta-1.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 1.6190 - activation_9_loss: 0.8353 - activation_10_loss: 0.7856 - val_loss: 1.6263 - val_activation_9_loss: 0.8334 - val_activation_10_loss: 0.7926\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4723 - activation_9_loss: 0.7601 - activation_10_loss: 0.7123\n",
      "Epoch 00003: val_loss did not improve from 1.62634\n",
      "21985/21985 [==============================] - 302s 14ms/sample - loss: 1.4723 - activation_9_loss: 0.7590 - activation_10_loss: 0.7113 - val_loss: 1.6698 - val_activation_9_loss: 0.8709 - val_activation_10_loss: 0.7985\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "0.7083525400644616\n",
      "3534/3534 [==============================] - 16s 5ms/sample\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.0670 - activation_11_loss: 1.0599 - activation_12_loss: 1.0071\n",
      "Epoch 00001: val_loss improved from inf to 1.69184, saving model to 1-roberta-2.h5\n",
      "21985/21985 [==============================] - 324s 15ms/sample - loss: 2.0669 - activation_11_loss: 1.0584 - activation_12_loss: 1.0057 - val_loss: 1.6918 - val_activation_11_loss: 0.8584 - val_activation_12_loss: 0.8335\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6254 - activation_11_loss: 0.8402 - activation_12_loss: 0.7852\n",
      "Epoch 00002: val_loss improved from 1.69184 to 1.67647, saving model to 1-roberta-2.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 1.6254 - activation_11_loss: 0.8419 - activation_12_loss: 0.7842 - val_loss: 1.6765 - val_activation_11_loss: 0.8707 - val_activation_12_loss: 0.8060\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4843 - activation_11_loss: 0.7674 - activation_12_loss: 0.7169\n",
      "Epoch 00003: val_loss improved from 1.67647 to 1.67127, saving model to 1-roberta-2.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 1.4842 - activation_11_loss: 0.7662 - activation_12_loss: 0.7159 - val_loss: 1.6713 - val_activation_11_loss: 0.8559 - val_activation_12_loss: 0.8155\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "0.703910398349325\n",
      "3534/3534 [==============================] - 17s 5ms/sample\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.0992 - activation_13_loss: 1.0829 - activation_14_loss: 1.0163\n",
      "Epoch 00001: val_loss improved from inf to 1.64704, saving model to 1-roberta-3.h5\n",
      "21985/21985 [==============================] - 323s 15ms/sample - loss: 2.0991 - activation_13_loss: 1.0814 - activation_14_loss: 1.0154 - val_loss: 1.6470 - val_activation_13_loss: 0.8342 - val_activation_14_loss: 0.8130\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6038 - activation_13_loss: 0.8243 - activation_14_loss: 0.7795\n",
      "Epoch 00002: val_loss improved from 1.64704 to 1.61602, saving model to 1-roberta-3.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 1.6038 - activation_13_loss: 0.8248 - activation_14_loss: 0.7784 - val_loss: 1.6160 - val_activation_13_loss: 0.8165 - val_activation_14_loss: 0.7995\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4707 - activation_13_loss: 0.7625 - activation_14_loss: 0.7082\n",
      "Epoch 00003: val_loss improved from 1.61602 to 1.61366, saving model to 1-roberta-3.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 1.4707 - activation_13_loss: 0.7614 - activation_14_loss: 0.7073 - val_loss: 1.6137 - val_activation_13_loss: 0.8271 - val_activation_14_loss: 0.7863\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 29s 5ms/sample\n",
      "0.7083767417838157\n",
      "3534/3534 [==============================] - 16s 5ms/sample\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n",
      "Train on 21985 samples, validate on 5496 samples\n",
      "Epoch 1/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 2.0691 - activation_15_loss: 1.0586 - activation_16_loss: 1.0105\n",
      "Epoch 00001: val_loss improved from inf to 1.69595, saving model to 1-roberta-4.h5\n",
      "21985/21985 [==============================] - 325s 15ms/sample - loss: 2.0690 - activation_15_loss: 1.0571 - activation_16_loss: 1.0090 - val_loss: 1.6960 - val_activation_15_loss: 0.8874 - val_activation_16_loss: 0.8078\n",
      "Epoch 2/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.6452 - activation_15_loss: 0.8460 - activation_16_loss: 0.7992\n",
      "Epoch 00002: val_loss improved from 1.69595 to 1.61761, saving model to 1-roberta-4.h5\n",
      "21985/21985 [==============================] - 303s 14ms/sample - loss: 1.6452 - activation_15_loss: 0.8448 - activation_16_loss: 0.7981 - val_loss: 1.6176 - val_activation_15_loss: 0.8401 - val_activation_16_loss: 0.7768\n",
      "Epoch 3/3\n",
      "21984/21985 [============================>.] - ETA: 0s - loss: 1.4968 - activation_15_loss: 0.7710 - activation_16_loss: 0.7258\n",
      "Epoch 00003: val_loss did not improve from 1.61761\n",
      "21985/21985 [==============================] - 302s 14ms/sample - loss: 1.4968 - activation_15_loss: 0.7725 - activation_16_loss: 0.7250 - val_loss: 1.6299 - val_activation_15_loss: 0.8580 - val_activation_16_loss: 0.7710\n",
      "Predicting OOF...\n",
      "5496/5496 [==============================] - 28s 5ms/sample\n",
      "0.7143255458290815\n",
      "3534/3534 [==============================] - 16s 5ms/sample\n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/conda/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "val_pred=dict({'start_token':np.zeros((train_encoded['input_id'].shape[0],MAX_LEN)),\n",
    "                      'end_token':np.zeros((train_encoded['input_id'].shape[0],MAX_LEN))})\n",
    "test_pred=dict({'start_token':np.zeros((test_encoded['input_id'].shape[0],MAX_LEN)),\n",
    "                      'end_token':np.zeros((test_encoded['input_id'].shape[0],MAX_LEN))})\n",
    "num_splits=5\n",
    "sss  = StratifiedKFold(n_splits=num_splits,shuffle=True,random_state=777)\n",
    "\n",
    "for fold,(train_index,val_index) in enumerate(sss.split(train_encoded['input_id'],train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    #K.clear_session()\n",
    "    model = build_model()\n",
    "        \n",
    "    sv = tf.keras.callbacks.ModelCheckpoint('%s-roberta-%i.h5'%(1,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "           save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "\n",
    "\n",
    "    X_train,y_train=[train_encoded['input_id'][train_index], train_encoded['attention_mask'][train_index],train_encoded['token_id'][train_index]], [train_encoded['start_token'][train_index], train_encoded['end_token'][train_index]]\n",
    "    X_val,y_val=[train_encoded['input_id'][val_index],train_encoded['attention_mask'][val_index],train_encoded['token_id'][val_index]], [train_encoded['start_token'][val_index], train_encoded['end_token'][val_index]]\n",
    "    test_data=[test_encoded['input_id'],test_encoded['attention_mask'],test_encoded['token_id']]\n",
    "\n",
    "    model.fit(X_train,y_train ,epochs=3, batch_size=32, verbose=1, callbacks=[sv], validation_data=(X_val,y_val))\n",
    "    \n",
    "    model.load_weights('%s-roberta-%i.h5'%(1,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    val_pred['start_token'][val_index,],val_pred['end_token'][val_index,] = model.predict(X_val,verbose=1)\n",
    "    sel_text_val=get_preds(train,val_pred,val_index,True)\n",
    "    \n",
    "    preds = model.predict(test_data,verbose=1)\n",
    "    test_pred['start_token']+=preds[0]/num_splits\n",
    "    test_pred['end_token']+=preds[1]/num_splits\n",
    "    \n",
    "sel_text_test=get_preds(test,test_pred,np.arange(0,test.shape[0]),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f87dea47db</td>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96d74cb729</td>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>eee518ae67</td>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text sentiment\n",
       "0  f87dea47db  Last session of the day  http://twitpic.com/67ezh   neutral\n",
       "1  96d74cb729   Shanghai is also really exciting (precisely -...  positive\n",
       "2  eee518ae67  Recession hit Veronique Branquinho, she has to...  negative"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>dabac40552</td>\n",
       "      <td>SUPER EXCITED. JUST BOUGHT MY TIX TO SEE HEY MONDAY ON ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>super excited.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>5a17107aba</td>\n",
       "      <td>So i have done absolutely NOTHING all day today...how pi...</td>\n",
       "      <td>negative</td>\n",
       "      <td>pitiful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>d39e6da95c</td>\n",
       "      <td>if you take backstreet boya and put them in silk mc ham...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>if you take backstreet boya and put them in silk mc ham...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3510</th>\n",
       "      <td>bbfe533366</td>\n",
       "      <td>In my moms hair salon, dying my hair</td>\n",
       "      <td>neutral</td>\n",
       "      <td>in my moms hair salon, dying my hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>51051cb758</td>\n",
       "      <td>lunch break`s over, going back to work....</td>\n",
       "      <td>neutral</td>\n",
       "      <td>lunch break`s over, going back to work....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3294</th>\n",
       "      <td>7a1556637a</td>\n",
       "      <td>That`s alright. Add your egg and maybe some sort of le...</td>\n",
       "      <td>positive</td>\n",
       "      <td>that`s alright. add your egg and maybe some sort of lef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2367</th>\n",
       "      <td>0f7696a955</td>\n",
       "      <td>Listening to maylene and wondering why cant I pull off a...</td>\n",
       "      <td>negative</td>\n",
       "      <td>wondering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>005e2e60d3</td>\n",
       "      <td>Now I need to change my monitor</td>\n",
       "      <td>neutral</td>\n",
       "      <td>now i need to change my monitor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>28db608c0d</td>\n",
       "      <td>Gonna go to work with my honey today!!!! So exciting!!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>so exciting!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>a172818c9e</td>\n",
       "      <td>misses my Mom today. She was my Best Friend and even tho...</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy mother`s day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>a154540c08</td>\n",
       "      <td>Haha and you should! State pride is important</td>\n",
       "      <td>positive</td>\n",
       "      <td>state pride is important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1107</th>\n",
       "      <td>d554ee2fe0</td>\n",
       "      <td>just got up, couldn`t sleep anymore i guess and im not r...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>just got up, couldn`t sleep anymore i guess and im not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>c2484278c9</td>\n",
       "      <td>120mm films usually handled at Fuji  Oh I met a girl at...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>02baa25e55</td>\n",
       "      <td>Oh no she stopped!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>oh no she stopped!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1773</th>\n",
       "      <td>2a69a25949</td>\n",
       "      <td>Don`t Chase Me - Shea Fisher download it</td>\n",
       "      <td>neutral</td>\n",
       "      <td>don`t chase me - shea fisher download it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>2cec07986b</td>\n",
       "      <td>Perth - Ford - Falcon - 1997 - $3,500  - new ad received...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>perth - ford - falcon - 1997 - $3,500 - new ad received...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>794</th>\n",
       "      <td>017a0bbb36</td>\n",
       "      <td>Aaaaahhhh.... Friday!!!! but.......     Funeral at 5</td>\n",
       "      <td>negative</td>\n",
       "      <td>funeral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>0b2ccf0b7e</td>\n",
       "      <td>lucky e went!  jealous!</td>\n",
       "      <td>negative</td>\n",
       "      <td>jealous!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1609</th>\n",
       "      <td>e7f2b44a1f</td>\n",
       "      <td>I totally forgot my phone at home this morning</td>\n",
       "      <td>negative</td>\n",
       "      <td>forgot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>84f961fdb2</td>\n",
       "      <td>Good Morning! 'RETURN TO WILLEN ISLAND - Spring has most...</td>\n",
       "      <td>positive</td>\n",
       "      <td>good morning! 'return to willen island - spring has mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>3d4b0a81a6</td>\n",
       "      <td>I think YOUR blog should`ve been on the list</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i think your blog should`ve been on the list</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>819626535b</td>\n",
       "      <td>Glad to hear you made it out, I hear that place used to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>b86fe0c61a</td>\n",
       "      <td>So tonights TV viewing, BGT or HIGNFY and Reggie Perrin?...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>so tonights tv viewing, bgt or hignfy and reggie perrin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3196</th>\n",
       "      <td>a028e284db</td>\n",
       "      <td>What timeeee? My mom says I have to do something daw to...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>what timeeee? my mom says i have to do something daw to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>ff7126ea72</td>\n",
       "      <td>gawww, why is facebook being so slow?</td>\n",
       "      <td>negative</td>\n",
       "      <td>gawww, why is facebook being so slow?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "2098  dabac40552   SUPER EXCITED. JUST BOUGHT MY TIX TO SEE HEY MONDAY ON ...   \n",
       "2986  5a17107aba  So i have done absolutely NOTHING all day today...how pi...   \n",
       "442   d39e6da95c   if you take backstreet boya and put them in silk mc ham...   \n",
       "3510  bbfe533366                         In my moms hair salon, dying my hair   \n",
       "970   51051cb758                   lunch break`s over, going back to work....   \n",
       "3294  7a1556637a    That`s alright. Add your egg and maybe some sort of le...   \n",
       "2367  0f7696a955  Listening to maylene and wondering why cant I pull off a...   \n",
       "251   005e2e60d3                              Now I need to change my monitor   \n",
       "933   28db608c0d      Gonna go to work with my honey today!!!! So exciting!!!   \n",
       "213   a172818c9e  misses my Mom today. She was my Best Friend and even tho...   \n",
       "951   a154540c08                Haha and you should! State pride is important   \n",
       "1107  d554ee2fe0  just got up, couldn`t sleep anymore i guess and im not r...   \n",
       "1499  c2484278c9   120mm films usually handled at Fuji  Oh I met a girl at...   \n",
       "1989  02baa25e55                                           Oh no she stopped!   \n",
       "1773  2a69a25949                     Don`t Chase Me - Shea Fisher download it   \n",
       "1954  2cec07986b  Perth - Ford - Falcon - 1997 - $3,500  - new ad received...   \n",
       "794   017a0bbb36         Aaaaahhhh.... Friday!!!! but.......     Funeral at 5   \n",
       "2260  0b2ccf0b7e                                      lucky e went!  jealous!   \n",
       "1609  e7f2b44a1f               I totally forgot my phone at home this morning   \n",
       "1465  84f961fdb2  Good Morning! 'RETURN TO WILLEN ISLAND - Spring has most...   \n",
       "2030  3d4b0a81a6                 I think YOUR blog should`ve been on the list   \n",
       "480   819626535b   Glad to hear you made it out, I hear that place used to...   \n",
       "1242  b86fe0c61a  So tonights TV viewing, BGT or HIGNFY and Reggie Perrin?...   \n",
       "3196  a028e284db   What timeeee? My mom says I have to do something daw to...   \n",
       "514   ff7126ea72                        gawww, why is facebook being so slow?   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "2098  positive                                               super excited.  \n",
       "2986  negative                                                      pitiful  \n",
       "442    neutral   if you take backstreet boya and put them in silk mc ham...  \n",
       "3510   neutral                         in my moms hair salon, dying my hair  \n",
       "970    neutral                   lunch break`s over, going back to work....  \n",
       "3294  positive   that`s alright. add your egg and maybe some sort of lef...  \n",
       "2367  negative                                                    wondering  \n",
       "251    neutral                              now i need to change my monitor  \n",
       "933   positive                                               so exciting!!!  \n",
       "213   positive                                           happy mother`s day  \n",
       "951   positive                                     state pride is important  \n",
       "1107   neutral   just got up, couldn`t sleep anymore i guess and im not ...  \n",
       "1499  negative                                                       i miss  \n",
       "1989   neutral                                           oh no she stopped!  \n",
       "1773   neutral                     don`t chase me - shea fisher download it  \n",
       "1954   neutral   perth - ford - falcon - 1997 - $3,500 - new ad received...  \n",
       "794   negative                                                      funeral  \n",
       "2260  negative                                                     jealous!  \n",
       "1609  negative                                                       forgot  \n",
       "1465  positive   good morning! 'return to willen island - spring has mos...  \n",
       "2030   neutral                 i think your blog should`ve been on the list  \n",
       "480   positive                                                         glad  \n",
       "1242   neutral   so tonights tv viewing, bgt or hignfy and reggie perrin...  \n",
       "3196   neutral   what timeeee? my mom says i have to do something daw to...  \n",
       "514   negative                        gawww, why is facebook being so slow?  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = sel_text_test\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
