{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Liverpool_ion_switch_RandomForest_ExtraTrees.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E9EjV93HCLBp",
        "M0TPIbavCTg8",
        "K_RpY1jDCX_U"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PViLMzrs195a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth=GoogleAuth()\n",
        "gauth.credentials=GoogleCredentials.get_application_default()\n",
        "\n",
        "drive=GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDYKD0YC2Cz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean = drive.CreateFile({'id': '1_y2mSjhcw0uaI_vTgrLdd0UScWIEsjIb'})\n",
        "train_clean.GetContentFile('train_clean.csv')\n",
        "test_clean = drive.CreateFile({'id': '1U0g_7u7g61UTywF0elODU06jyJ_wLfjF'})\n",
        "test_clean.GetContentFile('test_clean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQe7okcR2LWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = drive.CreateFile({'id': '1UAMpsYjI2161mmcs8MVsuEF-1okd43jT'})\n",
        "train.GetContentFile('train.csv')\n",
        "test = drive.CreateFile({'id': '17uPPXQbgzT772MCpsoddfEIYS95oGkD0'})\n",
        "test.GetContentFile('test.csv')\n",
        "sub = drive.CreateFile({'id': '1_48gjftaBNWfVd4DiMG4a7qQ-3Bv7P_N'})\n",
        "sub.GetContentFile('sample_submission.csv')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJb6nWtV2OCX",
        "colab_type": "code",
        "outputId": "71efc81e-a322-4b0a-96b6-d0865ba8cc98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "\n",
        "############################imports##########################################\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import tqdm\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split,KFold\n",
        "from sklearn.metrics import confusion_matrix, f1_score, plot_confusion_matrix\n",
        "from keras.models import Model\n",
        "import keras.layers as L\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from scipy import signal\n",
        "\n",
        "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier,HistGradientBoostingClassifier,GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "import json\n",
        "from google.colab import files\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ORFxO4Y2Qs6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_clean=pd.read_csv('train_clean.csv')\n",
        "test_clean=pd.read_csv('test_clean.csv')\n",
        "train=pd.read_csv('train.csv')\n",
        "test=pd.read_csv('test.csv')\n",
        "sub=pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnOxsQ4-4w5d",
        "colab_type": "code",
        "outputId": "489a63dd-c0ab-48d2-de27-d0ae4818cf13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "batch_separated=[];batch_separated_clean_train=[];batch_separated_clean_test=[]\n",
        "#train_clean['signal']=normalize(train_clean.signal)\n",
        "#test_clean['signal']=normalize(test_clean.signal)\n",
        "single_batch_size=500000\n",
        "num_of_batch_train=int(train.shape[0]/single_batch_size)\n",
        "num_of_batch_test=int(test.shape[0]/single_batch_size)\n",
        "\n",
        "train_clean['group']=0;test_clean['group']=0\n",
        "for i in range(num_of_batch_train):\n",
        "  train_clean['group'].iloc[i*single_batch_size:(i+1)*single_batch_size]=i\n",
        "  temp_df=train.iloc[single_batch_size*i:single_batch_size*(i+1)]\n",
        "  batch_separated.append(temp_df)\n",
        "  temp_df=train_clean.iloc[single_batch_size*i:single_batch_size*(i+1)]\n",
        "  batch_separated_clean_train.append(temp_df)\n",
        "for i in range(num_of_batch_test):\n",
        "  test_clean['group'].iloc[i*single_batch_size:(i+1)*single_batch_size]=i\n",
        "  temp_df=test_clean.iloc[single_batch_size*i:single_batch_size*(i+1)]\n",
        "  batch_separated_clean_test.append(temp_df)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:671: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self._setitem_with_indexer(indexer, value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpnV1cZP2WpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reduce_mem_usage(df, verbose=True):\n",
        "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
        "    start_mem = df.memory_usage().sum() / 1024**2    \n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtypes\n",
        "        if col_type in numerics:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int8)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)    \n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
        "    return df\n",
        "\n",
        "#train=reduce_mem_usage(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9EjV93HCLBp",
        "colab_type": "text"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxlBtv9d2XJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######## Feature generation#\n",
        "\n",
        "def feature_engineering(s,windows_roll,windows_ewm,windows_lag):\n",
        "  ewm = pd.DataFrame();high_pass = pd.DataFrame();low_pass = pd.DataFrame();grads = pd.DataFrame();merged_features=pd.DataFrame()\n",
        "  roll_stats = pd.DataFrame();lead_lag=pd.DataFrame();column_names=[]\n",
        "  n_filts=10\n",
        "  wns = np.logspace(-2, -0.3, n_filts)\n",
        "  n_grads=4\n",
        "  for window in windows_roll:\n",
        "        roll_stats['roll_mean_' + str(window)] = s.rolling(window=window, min_periods=1).mean()\n",
        "        roll_stats['roll_std_' + str(window)] = s.rolling(window=window, min_periods=1).std()\n",
        "        roll_stats['roll_min_' + str(window)] = s.rolling(window=window, min_periods=1).min()\n",
        "        roll_stats['roll_max_' + str(window)] = s.rolling(window=window, min_periods=1).max()\n",
        "        roll_stats['roll_range_' + str(window)] = roll_stats['roll_max_' + str(window)] - roll_stats['roll_min_' + str(window)]\n",
        "        roll_stats['roll_q10_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.10)\n",
        "        roll_stats['roll_q25_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.25)\n",
        "        roll_stats['roll_q50_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.50)\n",
        "        roll_stats['roll_q75_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.75)\n",
        "        roll_stats['roll_q90_' + str(window)] = s.rolling(window=window, min_periods=1).quantile(0.90)\n",
        "       # roll_stats['sig_mag' + str(window)] = s.rolling(window=window,min_periods=1).apply(lambda x: sum(abs(x))/single_batch_size)\n",
        "\n",
        "  roll_stats = roll_stats.fillna(value=0)\n",
        "  #print(roll_stats.shape)\n",
        "  roll_stats=roll_stats.reset_index(drop=True)\n",
        "  ################################################################################################################\n",
        "  for window in windows_lag:\n",
        "    lead_lag['lag_'+str(window)]=s.shift(window)\n",
        "    lead_lag['lead_'+str(window)]=s.shift(-1*window)\n",
        "  lead_lag=lead_lag.fillna(0)\n",
        "  lead_lag=lead_lag.reset_index(drop=True)\n",
        "\n",
        "  ################################################################################################################\n",
        "\n",
        "  g=s.values\n",
        "  for i in range(n_grads):\n",
        "    g = np.gradient(g)\n",
        "    grads['grad_' + str(i+1)] = g\n",
        "  grads=grads.fillna(0)\n",
        "  grads=grads.reset_index(drop=True)\n",
        "  ################################################################################################################\n",
        " \n",
        "  for wn in wns:\n",
        "    b, a = signal.butter(1, Wn=wn, btype='low')\n",
        "    low_pass['lowpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, s.values)\n",
        "    low_pass['lowpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, s.values)\n",
        "  low_pass=low_pass.fillna(0)\n",
        "  low_pass=low_pass.reset_index(drop=True)\n",
        "  ################################################################################################################\n",
        "  \n",
        "  for wn in wns:\n",
        "    b, a = signal.butter(1, Wn=wn, btype='high')\n",
        "    high_pass['hihgpass_lf_' + str('%.4f' %wn)] = signal.lfilter(b, a, s.values)\n",
        "    high_pass['hihgpass_ff_' + str('%.4f' %wn)] = signal.filtfilt(b, a, s.values)  \n",
        "  high_pass=high_pass.fillna(0)\n",
        "  high_pass=high_pass.reset_index(drop=True)\n",
        "  ################################################################################################################\n",
        "\n",
        "  for window in windows_ewm:\n",
        "    ewm['ewm_mean_' + str(window)] = s.ewm(span=window, min_periods=1).mean()\n",
        "    ewm['ewm_std_' + str(window)] = s.ewm(span=window, min_periods=1).std()\n",
        "  ewm=ewm.fillna(0)\n",
        "  ewm=ewm.reset_index(drop=True)\n",
        "  ################################################################################################################\n",
        "\n",
        "  merged_features=pd.concat([roll_stats,lead_lag,grads,ewm,low_pass,high_pass],axis=1,ignore_index=True) \n",
        "  #merged_features=merged_features.append([roll_stats,lead_lag,grads,ewm,low_pass,high_pass],)\n",
        "  #print(grads.shape,low_pass.shape,high_pass.shape,ewm.shape,lead_lag.shape,roll_stats.shape)\n",
        "  #print(\"aaa\")\n",
        "  #merged_features=pd.concat([lead_lag,grads],axis=1,ignore_index=True) \n",
        "\n",
        "  column_names=list(roll_stats.columns)+list(lead_lag.columns)+list(grads.columns)+list(ewm.columns)+list(low_pass.columns)+list(high_pass.columns)\n",
        "\n",
        "  #column_names=list(lead_lag.columns)+list(grads.columns)+list(ewm.columns)+list(low_pass.columns)+list(high_pass.columns)\n",
        "  #print(merged_features.shape)\n",
        "  return merged_features,column_names\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q453YLl72aJG",
        "colab_type": "code",
        "outputId": "20f24806-9692-41bf-b401-3686f07b5a49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "windows_roll=[10,50,100,500,1000];windows_ewm=[10,50];windows_lag=[1,2,3];new_features=pd.DataFrame()\n",
        "temp_df=pd.DataFrame()\n",
        "\n",
        "for i in tqdm.tqdm(range(num_of_batch_train)):  \n",
        "  temp_df,columns=feature_engineering(batch_separated_clean_train[i].signal,windows_roll,windows_ewm,windows_lag)\n",
        "  #new_features=pd.concat([new_features,temp_df],axis=0,ignore_index=True)\n",
        "  new_features=new_features.append(temp_df)\n",
        "  \n",
        "col_change=dict(zip(temp_df.columns,columns))\n",
        "new_features=reduce_mem_usage(new_features)\n",
        "new_features=new_features.reset_index(drop=True)\n",
        "train=pd.concat([train_clean,new_features],axis=1)\n",
        "train.rename(columns=col_change,inplace=True)\n",
        "train['sig2']=train['signal']**2\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [01:47<00:00, 10.80s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 1029.97 Mb (74.3% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0az2w9B2p35",
        "colab_type": "code",
        "outputId": "86f18d42-6c38-444a-e369-69f3495c774a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "new_features=pd.DataFrame();temp_df=pd.DataFrame()\n",
        "\n",
        "for i in tqdm.tqdm(range(num_of_batch_test)):  \n",
        "  temp_df,columns=feature_engineering(batch_separated_clean_test[i].signal,windows_roll,windows_ewm,windows_lag)\n",
        "  #new_features=pd.concat([new_features,temp_df],axis=0,ignore_index=True)\n",
        "  new_features=new_features.append(temp_df)\n",
        "  \n",
        "col_change=dict(zip(temp_df.columns,columns))\n",
        "new_features=reduce_mem_usage(new_features)\n",
        "new_features=new_features.reset_index(drop=True)\n",
        "test=pd.concat([test_clean,new_features],axis=1)\n",
        "test.rename(columns=col_change,inplace=True)\n",
        "test['sig2']=test['signal']**2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 4/4 [00:38<00:00,  9.64s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mem. usage decreased to 411.99 Mb (74.3% reduction)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0TPIbavCTg8",
        "colab_type": "text"
      },
      "source": [
        "# Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6KOAD0t2tJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpw9ppW_10LF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y=train.open_channels\n",
        "\n",
        "X=train.drop(['time','open_channels','group'],axis=1)\n",
        "FEATURES=X.columns\n",
        "feature_importances_df= pd.DataFrame(index=FEATURES)\n",
        "train_X,val_X,train_y,val_y =train_test_split(X,y,test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLwW6Ls_dOeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names=['rf','extra']\n",
        "models=[RandomForestClassifier(\n",
        "            n_estimators=10,\n",
        "            max_depth=19,\n",
        "            #max_features=10,\n",
        "            random_state=42,\n",
        "            n_jobs=10,\n",
        "            verbose=2),\n",
        "        ExtraTreesClassifier(\n",
        "            n_estimators=10,\n",
        "            max_depth=19,\n",
        "            #max_features=10,\n",
        "            random_state=42,\n",
        "            n_jobs=10,\n",
        "            verbose=2)]\n",
        "        \n",
        "        #HistGradientBoostingClassifier(loss='categorical_crossentropy')]\n",
        "classifiers_sel=dict(zip(names,models))\n",
        "selected_features=dict(zip(names,[]))\n",
        "cc=np.zeros((len(names),test.shape[0],train.open_channels.nunique()))\n",
        "preds_=dict(zip(names,cc))\n",
        "train_X,val_X,train_y,val_y =train_test_split(X,y,test_size=0.2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyER-PuwjBuZ",
        "colab_type": "code",
        "outputId": "876a497a-242e-4f28-dde7-d22922917292",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        }
      },
      "source": [
        "for i in range(len(names)):\n",
        "  classifiers_sel[names[i]].fit(train_X, train_y)\n",
        "  f1=f1_score(val_y,classifiers_sel[names[i]].predict(val_X),average='macro')\n",
        "  print(\"F1-Score for classifier \"+names[i],\" : \",f1)\n",
        "  feature_importances_df['importance'] = classifiers_sel[names[i]].feature_importances_\n",
        "  feature_importances_df=feature_importances_df.sort_values(by='importance',ascending=False)\n",
        "  selected_features[names[i]]=feature_importances_df.index[:30]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  4.5min remaining: 10.5min\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  5.3min finished\n",
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.4s remaining:    3.3s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for classifier rf  :  0.9366018873248794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10building tree 2 of 10\n",
            "\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  1.2min remaining:  2.8min\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  1.4min finished\n",
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.6s remaining:    3.6s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.7s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for classifier extra  :  0.9352787486172226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_RpY1jDCX_U",
        "colab_type": "text"
      },
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c3LiOc7efWG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models=[RandomForestClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=19,\n",
        "            #max_features=10,\n",
        "            random_state=42,\n",
        "            n_jobs=10,\n",
        "            verbose=2),\n",
        "        ExtraTreesClassifier(\n",
        "            n_estimators=100,\n",
        "            max_depth=19,\n",
        "            #max_features=10,\n",
        "            random_state=42,\n",
        "            n_jobs=10,\n",
        "            verbose=2)]\n",
        "classifiers_sel=dict(zip(names,models))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moVh_QZRSMYt",
        "colab_type": "code",
        "outputId": "70b39536-3ee6-4741-e01d-b8d7f4af5e1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "folds=5\n",
        "kf = KFold(n_splits=folds, shuffle=True, random_state=667)\n",
        "fold=1\n",
        "models = []; preds_net=[]\n",
        "for i in range(len(names)):\n",
        "    preds_ = np.zeros((len(test), 11))\n",
        "    print(f'====== Fold {fold:0.0f} of {folds} ======')\n",
        "    for train_index, val_index in kf.split(X, y):\n",
        "    \n",
        "      train_X = X[selected_features[names[i]]].iloc[train_index]\n",
        "      val_X = X[selected_features[names[i]]].iloc[val_index]\n",
        "      train_y = y.iloc[train_index]\n",
        "      val_y = y.iloc[val_index]\n",
        "    \n",
        "      classifiers_sel[names[i]].fit(train_X, train_y)\n",
        "      f1=f1_score(val_y,classifiers_sel[names[i]].predict(val_X),average='macro')\n",
        "      print(f\"F1-Score for classifier of fold {fold} \"+names[i],\" : \",f1)\n",
        "      preds_proba = classifiers_sel[names[i]].predict_proba(test[selected_features[names[i]]])\n",
        "      preds_proba = preds_proba.reshape(-1, preds_proba.shape[-1])  \n",
        "      print(type(preds_proba),preds_proba.shape)         \n",
        "      preds_+=preds_proba / folds\n",
        "    preds_net.append(preds_)  \n",
        "    fold+=1\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "====== Fold 1 of 2 ======\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  1.1min remaining:  2.6min\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  1.2min finished\n",
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    2.0s remaining:    4.7s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    2.4s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for classifier of fold 1 rf  :  0.9357309882861263\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.3s remaining:    3.0s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (2000000, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10building tree 2 of 10building tree 3 of 10\n",
            "\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:  1.1min remaining:  2.6min\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:  1.1min finished\n",
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.9s remaining:    4.3s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    2.3s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for classifier of fold 1 rf  :  0.9360648072186581\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.3s remaining:    3.0s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (2000000, 11)\n",
            "====== Fold 2 of 2 ======\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10\n",
            "building tree 7 of 10building tree 8 of 10building tree 9 of 10\n",
            "building tree 10 of 10\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   15.0s remaining:   35.0s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   15.5s finished\n",
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    2.1s remaining:    5.0s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    2.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for classifier of fold 2 extra  :  0.927467118788783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.3s remaining:    3.1s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.6s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (2000000, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building tree 1 of 10\n",
            "building tree 2 of 10\n",
            "building tree 3 of 10\n",
            "building tree 4 of 10\n",
            "building tree 5 of 10\n",
            "building tree 6 of 10building tree 7 of 10\n",
            "building tree 8 of 10\n",
            "\n",
            "building tree 9 of 10\n",
            "building tree 10 of 10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:   14.9s remaining:   34.7s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:   15.7s finished\n",
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    2.2s remaining:    5.0s\n",
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    2.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "F1-Score for classifier of fold 2 extra  :  0.9272122713725359\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend ThreadingBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   3 out of  10 | elapsed:    1.3s remaining:    3.1s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.ndarray'> (2000000, 11)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Done  10 out of  10 | elapsed:    1.7s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmGB4tiHTjIF",
        "colab_type": "code",
        "outputId": "9cc18134-2731-4f32-d6a4-f4a0104fa68e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "preds_net[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000000, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RWGIdFMn7PPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub['open_channels'] = np.argmax(preds_net[0], axis = 1).astype(int)\n",
        "sub.to_csv('submission_rrrrrrrrrrrf.csv', index=False, float_format='%.4f')\n",
        "files.download('submission_rrrrrrrrrrrf.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6MD1evrqpXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sub['open_channels'] = np.argmax(preds_net[1], axis = 1).astype(int)\n",
        "sub.to_csv('submission_extra_trees.csv', index=False, float_format='%.4f')\n",
        "files.download('submission_extra_trees.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0evURchw1ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}